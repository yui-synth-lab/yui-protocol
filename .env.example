# Yui Protocol - AI Model API Keys
# Copy this file to .env and fill in your actual API keys

# OpenAI API Key
OPENAI_API_KEY=your_openai_api_key_here

# Anthropic (Claude) API Key
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Google Gemini API Key
GEMINI_API_KEY=your_gemini_api_key_here

# Ollama Base URL (optional, defaults to http://localhost:11434)
OLLAMA_BASE_URL=http://localhost:11434

# llama.cpp Server Base URL (optional, defaults to http://localhost:8080)
# Used when provider is 'llamacpp' (requires llama-server running)
LLAMACPP_BASE_URL=http://localhost:8080

# llama.cpp Model Name (optional, can be any identifier)
# This is used as the model name when making requests to llama.cpp server
LLAMACPP_MODEL_NAME=local-model

# ============================================================================
# llama.cpp Local Execution Settings (for llamacpp-local provider)
# ============================================================================
# These settings are used when provider is 'llamacpp-local'
# This mode loads GGUF models directly without requiring llama-server

# Path to the GGUF model file (required for llamacpp-local)
# Example: LLAMACPP_MODEL_PATH=C:/models/llama-3.3-70b-instruct-q4_k_m.gguf
LLAMACPP_MODEL_PATH=

# Context size (optional, defaults to 4096)
# Larger values allow longer conversations but use more memory
LLAMACPP_CONTEXT_SIZE=4096

# Number of GPU layers (optional, defaults to 0)
# 0 = CPU only, higher values = more layers offloaded to GPU
# Requires CUDA/Metal support. Set to -1 to offload all layers.
LLAMACPP_GPU_LAYERS=0

# ============================================================================
# Agent-Specific Model Configuration (Optional)
# ============================================================================
# Override each agent's model configuration defined in their agent files.
# Format: AGENT_<AGENT_ID>_PROVIDER and AGENT_<AGENT_ID>_MODEL
# Leave blank to use the default configuration from agent files.

# yui-000 (結心) - Default: openai/gpt-5-mini
# AGENT_YUI_000_PROVIDER=openai
# AGENT_YUI_000_MODEL=gpt-5-mini-2025-08-07
# AGENT_YUI_000_FINALIZER_MODEL=gpt-5.2-2025-12-11

# eiro-001 (慧露) - Default: openai/gpt-5-mini
# AGENT_EIRO_001_PROVIDER=openai
# AGENT_EIRO_001_MODEL=gpt-5-mini-2025-08-07
# AGENT_EIRO_001_FINALIZER_MODEL=gpt-5.2-2025-12-11

# kanshi-001 (観至) - Default: anthropic/claude-haiku-4-5
# AGENT_KANSHI_001_PROVIDER=anthropic
# AGENT_KANSHI_001_MODEL=claude-haiku-4-5-20251001
# AGENT_KANSHI_001_FINALIZER_MODEL=claude-sonnet-4-5-20250929

# hekito-001 (碧統) - Default: gemini/gemini-3-flash
# AGENT_HEKITO_001_PROVIDER=gemini
# AGENT_HEKITO_001_MODEL=gemini-3-flash-preview
# AGENT_HEKITO_001_FINALIZER_MODEL=gemini-3-pro-preview

# yoga-001 (陽雅) - Default: anthropic/claude-haiku-4-5
# AGENT_YOGA_001_PROVIDER=anthropic
# AGENT_YOGA_001_MODEL=claude-haiku-4-5-20251001
# AGENT_YOGA_001_FINALIZER_MODEL=claude-sonnet-4-5-20250929

# facilitator-001 (ファシリテーター) - Default: openai/gpt-4o
# AGENT_FACILITATOR_001_PROVIDER=openai
# AGENT_FACILITATOR_001_MODEL=gpt-4o

# ============================================================================
# Local Model Configuration Examples
# ============================================================================
# To use llama.cpp server for any agent, set the provider to 'llamacpp'
# (Requires llama-server running on port 8080)
# Example for yui-000:
# AGENT_YUI_000_PROVIDER=llamacpp
# AGENT_YUI_000_MODEL=local-model
# AGENT_YUI_000_FINALIZER_MODEL=local-model

# To use llama.cpp local execution (direct GGUF loading), set the provider to 'llamacpp-local'
# (No server required, loads model directly in Node.js)
# Example for kanshi-001:
# AGENT_KANSHI_001_PROVIDER=llamacpp-local
# AGENT_KANSHI_001_MODEL=llama-3.3-70b-instruct

# To use Ollama for any agent, set the provider to 'ollama'
# Example for eiro-001:
# AGENT_EIRO_001_PROVIDER=ollama
# AGENT_EIRO_001_MODEL=llama3.3:latest
# AGENT_EIRO_001_FINALIZER_MODEL=llama3.3:latest

# Memory Management Models
MEMORY_COMPRESSOR_MODEL=gpt-4o-mini
MEMORY_MANAGER_MODEL=gpt-4o-mini

# Stage Summarizer (v1.0) - Default: gemini/gemini-3-flash-preview
STAGE_SUMMARIZER_PROVIDER=gemini
STAGE_SUMMARIZER_MODEL=gemini-3-flash-preview
